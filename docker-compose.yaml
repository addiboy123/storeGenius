version: '3.8'

services:
  ollama:
    image: ollama/ollama
    ports:
      - "11433:11434"  # Keep your custom external:internal port mapping
    volumes:
      - ollama-data:/root/.ollama
      - ./Modelfile:/Modelfile
    environment:
      - OLLAMA_HOST=0.0.0.0:11434  # Make sure server listens on internal port
    entrypoint: >
      sh -c "ollama serve & 
             sleep 10 && 
             [ -f /root/.ollama/models/blobs/manifest-storeGeniusLLM ] || 
             (OLLAMA_HOST=http://localhost:11434 ollama create storeGeniusLLM -f /Modelfile) && 
             wait"
    restart: unless-stopped

  storegenius-api:
    build:
      context: ./ML
    ports:
      - "5050:5050"
    depends_on:
      - ollama
      - backend
    environment:
      - OLLAMA_HOST=http://ollama:11434
      - BACKEND_URL=http://backend:7000
    restart: unless-stopped

  frontend:
    build:
      context: ./project
      dockerfile: Dockerfile
    ports:
      - "5173:5173"
    depends_on:
      - backend
      - storegenius-api
    environment:
      VITE_BACKEND_URL: http://backend:7000
      VITE_API_URL: http://storegenius-api:5050
      VITE_GEMINI_API: ${VITE_GEMINI_API}
    restart: unless-stopped

  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
    ports:
      - "7000:7000"
    environment:
      SECRET_KEY: ${SECRET_KEY}
      API_KEY: ${API_KEY}
      PORT: ${PORT}
      MONGO_URI: ${MONGO_URI}
      API_URL: http://storegenius-api:5050
    restart: unless-stopped

volumes:
  ollama-data:
